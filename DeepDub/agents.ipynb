{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Using cached openai-1.57.4-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/DeepDub/lib/python3.9/site-packages (from openai) (4.6.2.post1)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/DeepDub/lib/python3.9/site-packages (from openai) (0.27.2)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.8.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/envs/DeepDub/lib/python3.9/site-packages (from openai) (2.10.1)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/DeepDub/lib/python3.9/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/envs/DeepDub/lib/python3.9/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/envs/DeepDub/lib/python3.9/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/envs/DeepDub/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/anaconda3/envs/DeepDub/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/DeepDub/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/DeepDub/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/DeepDub/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/DeepDub/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/anaconda3/envs/DeepDub/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
      "Using cached openai-1.57.4-py3-none-any.whl (390 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.8.2-cp39-cp39-macosx_11_0_arm64.whl (300 kB)\n",
      "Installing collected packages: jiter, distro, openai\n",
      "Successfully installed distro-1.9.0 jiter-0.8.2 openai-1.57.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    api_key = 'lms'\n",
    ")\n",
    "\n",
    "class PreprocessingAgent:\n",
    "    def __init__(self):\n",
    "        self.units_regex = r\"\\d+\\s*(miles|kilometers|pounds|kilograms)\"\n",
    "\n",
    "    def identify_critical_segments(self, dialogue):\n",
    "        # Rule-based unit detection\n",
    "        critical_entities = []\n",
    "        if re.search(self.units_regex, dialogue):\n",
    "            critical_entities.append(\"Unit Conversion\")\n",
    "\n",
    "        # OpenAI-based detection of expressions\n",
    "        expressions = self.detect_expressions(dialogue)\n",
    "        if expressions:\n",
    "            critical_entities.extend([f\"Expression: {expr}\" for expr in expressions])\n",
    "\n",
    "        # OpenAI-based cultural reference detection\n",
    "        cultural_reference = self.detect_cultural_reference(dialogue)\n",
    "        if cultural_reference:\n",
    "            critical_entities.append(f\"Cultural Reference: {cultural_reference}\")\n",
    "\n",
    "        if critical_entities:\n",
    "            return f\"Detected Critical Entities: {', '.join(critical_entities)}\"\n",
    "\n",
    "        return \"Standard\"\n",
    "\n",
    "    def detect_cultural_reference(self, dialogue):\n",
    "        # LLM-based detection with OpenAI GPT model\n",
    "        prompt = f\"Analyze the following text and identify if it contains any cultural references or expressions. If so, explain:\\n\\n\\\"{dialogue}\\\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in detecting cultural references in text.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    def detect_expressions(self, dialogue):\n",
    "        # Use OpenAI GPT model to detect expressions dynamically\n",
    "        prompt = f\"Analyze the following text and list all idiomatic expressions or common phrases:\\n\\n\\\"{dialogue}\\\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in identifying idiomatic expressions.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        expressions = response.choices[0].message.content.strip()\n",
    "        return [expr.strip() for expr in expressions.split(',') if expr.strip()]\n",
    "\n",
    "class TranslationAgent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def translate(self, dialogue):\n",
    "        # Use OpenAI GPT model for translation\n",
    "        prompt = f\"Translate the following English text into French:\\n\\n\\\"{dialogue}\\\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert translator from English to French.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "class ContextualAgent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def adapt_context(self, dialogue):\n",
    "        # Use OpenAI GPT model for contextual adaptation\n",
    "        prompt = f\"Refine the following French translation to better fit the context:\\n\\n\\\"{dialogue}\\\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in refining translations for better contextual accuracy.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "class ManualValidationAgent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def validate(self, original, contextual):\n",
    "        print(\"Original:\", original)\n",
    "        print(\"Contextual Translation:\", contextual)\n",
    "        user_input = input(\"Enter your validation or press Enter to accept: \")\n",
    "        return user_input if user_input.strip() else contextual\n",
    "\n",
    "class OptimizationAgent:\n",
    "    def __init__(self, max_chars=42):\n",
    "        self.max_chars = max_chars\n",
    "\n",
    "    def optimize(self, dialogue):\n",
    "        if len(dialogue) > self.max_chars:\n",
    "            return dialogue[:self.max_chars-3] + \"...\"\n",
    "        return dialogue\n",
    "\n",
    "class DialogueTranslationPipeline:\n",
    "    def __init__(self):\n",
    "        self.preprocessing_agent = PreprocessingAgent()\n",
    "        self.translation_agent = TranslationAgent()\n",
    "        self.contextual_agent = ContextualAgent()\n",
    "        self.manual_validation_agent = ManualValidationAgent()\n",
    "        self.optimization_agent = OptimizationAgent()\n",
    "\n",
    "    def process_dialogue(self, dialogue):\n",
    "        # Step 1: Preprocessing\n",
    "        critical_type = self.preprocessing_agent.identify_critical_segments(dialogue)\n",
    "        print(f\"Critical Segment Type: {critical_type}\")\n",
    "\n",
    "        # Step 2: Translation\n",
    "        translated = self.translation_agent.translate(dialogue)\n",
    "        print(f\"Translated: {translated}\")\n",
    "\n",
    "        # Step 3: Contextual Adaptation\n",
    "        contextual = self.contextual_agent.adapt_context(translated)\n",
    "        print(f\"Contextual Translation: {contextual}\")\n",
    "\n",
    "        # Step 4: Manual Validation\n",
    "        validated = self.manual_validation_agent.validate(dialogue, contextual)\n",
    "\n",
    "        # Step 5: Optimization\n",
    "        optimized = self.optimization_agent.optimize(validated)\n",
    "        print(f\"Final Optimized Translation: {optimized}\")\n",
    "\n",
    "        return optimized\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical Segment Type: Detected Critical Entities: Expression: In the provided text, Expression: there are two idiomatic expressions:\n",
      "\n",
      "1. **\"A piece of cake\"**: This idiom means that something is very easy to accomplish.\n",
      "\n",
      "2. **\"Break the ice\"**: This expression refers to doing or saying something to relieve tension or get a conversation started in a social setting.\n",
      "\n",
      "These phrases are commonly used to convey ease and the initiation of interaction, Expression: respectively., Cultural Reference: The text you provided does contain cultural references and expressions that are commonly used in English-speaking contexts.\n",
      "\n",
      "1. **\"The meeting was a piece of cake\":** This is an idiomatic expression meaning that something was very easy to accomplish or required little effort. The phrase \"piece of cake\" originates from the notion that eating cake is enjoyable and simple, thus likening an easy task to this experience. It reflects a cultural tendency in English to use food-related metaphors for ease and simplicity.\n",
      "\n",
      "2. **\"He's gonna break the ice\":** This is another idiomatic expression meaning that someone will initiate conversation or activity to create a more relaxed atmosphere among people who may be feeling awkward, reserved, or unfamiliar with one another. The origin of this phrase likely comes from the idea of breaking through literal ice to clear a path for navigation or communication, metaphorically applying it to social interactions.\n",
      "\n",
      "Both expressions are widely recognized in English-speaking cultures and are used frequently in both formal and informal contexts.\n",
      "Translated: La réunion s'est déroulée sans problème et il va briser la glace.\n",
      "Contextual Translation: To refine this translation and ensure it fits the context more accurately, we should consider what \"break the ice\" typically means in English. It often refers to initiating conversation or easing tension among people who might be unfamiliar with each other. In a professional setting, like a meeting, it could mean starting the formal proceedings or making an initial announcement.\n",
      "\n",
      "Here's a refined translation:\n",
      "\n",
      "\"La réunion s'est déroulée sans problème et il va maintenant démarrer les discussions.\"\n",
      "\n",
      "This version suggests that after the trouble-free start of the meeting, he will begin to engage participants in conversation or discussion. This captures the idea of \"breaking the ice\" more contextually within a meeting setting.\n",
      "Original: The meeting was a piece of cake, and he's gonna break the ice.\n",
      "Contextual Translation: To refine this translation and ensure it fits the context more accurately, we should consider what \"break the ice\" typically means in English. It often refers to initiating conversation or easing tension among people who might be unfamiliar with each other. In a professional setting, like a meeting, it could mean starting the formal proceedings or making an initial announcement.\n",
      "\n",
      "Here's a refined translation:\n",
      "\n",
      "\"La réunion s'est déroulée sans problème et il va maintenant démarrer les discussions.\"\n",
      "\n",
      "This version suggests that after the trouble-free start of the meeting, he will begin to engage participants in conversation or discussion. This captures the idea of \"breaking the ice\" more contextually within a meeting setting.\n",
      "Final Optimized Translation: To refine this translation and ensure i...\n",
      "Result: To refine this translation and ensure i...\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "pipeline = DialogueTranslationPipeline()\n",
    "dialogue = \"The meeting was a piece of cake, and he's gonna break the ice.\"\n",
    "result = pipeline.process_dialogue(dialogue)\n",
    "print(\"Result:\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepDub",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
